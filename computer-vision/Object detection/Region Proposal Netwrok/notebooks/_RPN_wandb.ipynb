{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6b08e84-d116-4783-8cf9-815a0e5ad432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>loss</td><td>0.04534</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rpn_resnet18_run</strong> at: <a href='https://wandb.ai/hsimchi74-shahid-behesti-university/rpn-training/runs/6f2wzxbp' target=\"_blank\">https://wandb.ai/hsimchi74-shahid-behesti-university/rpn-training/runs/6f2wzxbp</a><br> View project at: <a href='https://wandb.ai/hsimchi74-shahid-behesti-university/rpn-training' target=\"_blank\">https://wandb.ai/hsimchi74-shahid-behesti-university/rpn-training</a><br>Synced 4 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251208_101246-6f2wzxbp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hossein-simchi/ML/computer-vision/Object detection/Region Proposal Netwrok/notebooks/wandb/run-20251208_101520-dnh70r7b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hsimchi74-shahid-behesti-university/rpn-training/runs/dnh70r7b' target=\"_blank\">rpn_resnet18_run</a></strong> to <a href='https://wandb.ai/hsimchi74-shahid-behesti-university/rpn-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hsimchi74-shahid-behesti-university/rpn-training' target=\"_blank\">https://wandb.ai/hsimchi74-shahid-behesti-university/rpn-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hsimchi74-shahid-behesti-university/rpn-training/runs/dnh70r7b' target=\"_blank\">https://wandb.ai/hsimchi74-shahid-behesti-university/rpn-training/runs/dnh70r7b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary saved and logged to WandB: /home/hossein-simchi/ML/computer-vision/Object detection/Region Proposal Netwrok/notebooks/wandb/run-20251208_101520-dnh70r7b/files/model_outputs/model_summary.txt\n",
      "Epoch 1 | Loss: 0.0453\n",
      "Epoch 2 | Loss: 0.0212\n",
      "Epoch 3 | Loss: 0.0178\n",
      "Epoch 4 | Loss: 0.0160\n",
      "Epoch 5 | Loss: 0.0147\n",
      "Saved and logged proposal visualization to: /home/hossein-simchi/ML/computer-vision/Object detection/Region Proposal Netwrok/notebooks/wandb/run-20251208_101520-dnh70r7b/files/rpn_proposals/image_0002_rpn_proposals.png\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# ----------------------------\n",
    "#  IMPORTS\n",
    "# ----------------------------\n",
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.models.detection.image_list import ImageList\n",
    "from torchsummary import summary\n",
    "import io\n",
    "from torchvision.models.detection.rpn import RPNHead, RegionProposalNetwork, AnchorGenerator\n",
    "import wandb\n",
    "\n",
    "# ----------------------------\n",
    "#  DEVICE\n",
    "# ----------------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ----------------------------\n",
    "#  PATHS & CONSTANTS\n",
    "# ----------------------------\n",
    "IMAGE_DIR = '../dataset/images'\n",
    "ANNOTATIONS_DIR = '../dataset/annotations'\n",
    "TARGET_SIZE = (224, 224)\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# ----------------------------\n",
    "#  INIT WandB\n",
    "# ----------------------------\n",
    "wandb.init(\n",
    "    project=\"rpn-training\",\n",
    "    name=\"rpn_resnet18_run\",\n",
    "    config={\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"lr\": 0.001,\n",
    "        \"backbone\": \"resnet18\",\n",
    "        \"input_size\": \"224x224\"\n",
    "    },\n",
    "    settings=wandb.Settings(init_timeout=120)\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "#  LOAD DATA\n",
    "# ----------------------------\n",
    "class_names = [d for d in os.listdir(IMAGE_DIR) if os.path.isdir(os.path.join(IMAGE_DIR, d))]\n",
    "csv_files = [f for f in os.listdir(ANNOTATIONS_DIR) if f.endswith('.csv')]\n",
    "label_map = {name: i + 1 for i, name in enumerate(class_names)}\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for i in range(len(class_names)):\n",
    "    class_name = class_names[i]\n",
    "    class_dir = os.path.join(IMAGE_DIR, class_name)\n",
    "    csv_file_name = csv_files[i]\n",
    "    csv_path = os.path.join(ANNOTATIONS_DIR, csv_file_name)\n",
    "    df_annotations = pd.read_csv(csv_path)\n",
    "\n",
    "    for image_name in os.listdir(class_dir):\n",
    "        image_path = os.path.join(class_dir, image_name)\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            continue\n",
    "\n",
    "        h, w, _ = image.shape\n",
    "        row = df_annotations[df_annotations['image_name'] == image_name]\n",
    "        if row.empty:\n",
    "            continue\n",
    "\n",
    "        ann = row.iloc[0, 1:].tolist()\n",
    "        ann[0] = (ann[0] / w) * 224\n",
    "        ann[1] = (ann[1] / h) * 224\n",
    "        ann[2] = (ann[2] / w) * 224\n",
    "        ann[3] = (ann[3] / h) * 224\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, TARGET_SIZE)\n",
    "\n",
    "        image_tensor = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "        label_tensor = torch.tensor([class_names.index(class_name)], dtype=torch.int64)\n",
    "        ann_tensor = torch.tensor([ann], dtype=torch.float32)\n",
    "\n",
    "        target = {'boxes': ann_tensor, 'labels': label_tensor}\n",
    "        dataset.append((image_tensor, target))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# ----------------------------\n",
    "#  BACKBONE (ResNet18)\n",
    "# ----------------------------\n",
    "resnet_model = torchvision.models.resnet18()\n",
    "backbone = torch.nn.Sequential(*list(resnet_model.children())[:-2])\n",
    "backbone.out_channels = 512\n",
    "\n",
    "# ----------------------------\n",
    "#  SAVE MODEL SUMMARY\n",
    "# ----------------------------\n",
    "# Make WandB output dir\n",
    "wandb_output_dir = os.path.join(wandb.run.dir, \"model_outputs\")\n",
    "os.makedirs(wandb_output_dir, exist_ok=True)\n",
    "\n",
    "summary_path = os.path.join(wandb_output_dir, \"model_summary.txt\")\n",
    "buffer = io.StringIO()\n",
    "sys_stdout = sys.stdout\n",
    "sys.stdout = buffer\n",
    "summary(backbone, (3, 224, 224))\n",
    "sys.stdout = sys_stdout\n",
    "\n",
    "with open(summary_path, \"w\") as f:\n",
    "    f.write(buffer.getvalue())\n",
    "\n",
    "# Log summary as artifact\n",
    "artifact = wandb.Artifact(\"model_summary\", type=\"summary\")\n",
    "artifact.add_file(summary_path)\n",
    "wandb.log_artifact(artifact)\n",
    "print(\"Model summary saved and logged to WandB:\", summary_path)\n",
    "\n",
    "# Freeze backbone parameters\n",
    "for param in backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# ----------------------------\n",
    "#  RPN MODEL\n",
    "# ----------------------------\n",
    "anchor_generator = AnchorGenerator(\n",
    "    sizes=((32, 64, 128),),\n",
    "    aspect_ratios=((0.5, 1.0, 2.0),)\n",
    ")\n",
    "in_channels = backbone.out_channels\n",
    "num_anchors = anchor_generator.num_anchors_per_location()[0]\n",
    "rpn_head = RPNHead(in_channels=in_channels, num_anchors=num_anchors)\n",
    "\n",
    "rpn_model = RegionProposalNetwork(\n",
    "    anchor_generator,\n",
    "    rpn_head,\n",
    "    fg_iou_thresh=0.7,\n",
    "    bg_iou_thresh=0.3,\n",
    "    batch_size_per_image=256,\n",
    "    positive_fraction=0.5,\n",
    "    pre_nms_top_n={'training': 2000, 'testing': 1000},\n",
    "    post_nms_top_n={'training': 1000, 'testing': 500},\n",
    "    nms_thresh=0.7\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(rpn_model.parameters(), lr=0.001)\n",
    "\n",
    "# ----------------------------\n",
    "#  TRAINING LOOP\n",
    "# ----------------------------\n",
    "log_path = os.path.join(wandb_output_dir, \"training_log.txt\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_losses = []\n",
    "\n",
    "    for images, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        images_gpu = torch.stack([img.to(DEVICE) for img in images])\n",
    "        targets_gpu = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            features = backbone(images_gpu)\n",
    "\n",
    "        image_list = ImageList(images_gpu, [img.shape[-2:] for img in images])\n",
    "        _, loss_dict = rpn_model(image_list, {'0': features}, targets_gpu)\n",
    "        loss = loss_dict['loss_objectness'] + loss_dict['loss_rpn_box_reg']\n",
    "\n",
    "        if torch.isfinite(loss):\n",
    "            epoch_losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    mean_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    log_line = f\"Epoch {epoch+1} | Loss: {mean_loss:.4f}\\n\"\n",
    "    print(log_line.strip())\n",
    "\n",
    "    with open(log_path, \"a\") as f:\n",
    "        f.write(log_line)\n",
    "\n",
    "    wandb.log({\"epoch\": epoch + 1, \"loss\": mean_loss})\n",
    "\n",
    "    # Save checkpoint and log as artifact\n",
    "    checkpoint_path = os.path.join(wandb_output_dir, f\"rpn_epoch_{epoch+1}.pth\")\n",
    "    torch.save(rpn_model.state_dict(), checkpoint_path)\n",
    "    ckpt_artifact = wandb.Artifact(f\"rpn_checkpoint_epoch_{epoch+1}\", type=\"model\")\n",
    "    ckpt_artifact.add_file(checkpoint_path)\n",
    "    wandb.log_artifact(ckpt_artifact)\n",
    "\n",
    "# ----------------------------\n",
    "#  VISUALIZATION FUNCTION\n",
    "# ----------------------------\n",
    "def visualize_rpn_proposals(image_path, rpn_model_trained, backbone_model_trained):\n",
    "    proposals_dir = os.path.join(wandb.run.dir, \"rpn_proposals\")\n",
    "    os.makedirs(proposals_dir, exist_ok=True)\n",
    "\n",
    "    rpn_model_trained.to(DEVICE)\n",
    "    backbone_model_trained.to(DEVICE)\n",
    "    rpn_model_trained.eval()\n",
    "    backbone_model_trained.eval()\n",
    "\n",
    "    img_bgr = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    image_rgb_resized = cv2.resize(img_rgb, TARGET_SIZE)\n",
    "    img_tensor = (torch.tensor(image_rgb_resized, dtype=torch.float32).permute(2, 0, 1) / 255.0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = backbone_model_trained(img_tensor.unsqueeze(0))\n",
    "        image_list = ImageList(img_tensor.unsqueeze(0), [tuple(img_tensor.shape[-2:])])\n",
    "        proposals, _ = rpn_model_trained(image_list, {'0': features})\n",
    "\n",
    "    top_proposals = proposals[0][:5].cpu().numpy()\n",
    "    img_display = image_rgb_resized.copy()\n",
    "\n",
    "    for i, box in enumerate(top_proposals):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        color = (0,255,0) if i==0 else ((255,255,0) if i < 3 else (255,0,0))\n",
    "        width = 3 if i==0 else (2 if i<3 else 1)\n",
    "        cv2.rectangle(img_display, (x1, y1), (x2, y2), color, width)\n",
    "\n",
    "    base_name = os.path.basename(image_path).split('.')[0]\n",
    "    output_path = os.path.join(proposals_dir, f\"{base_name}_rpn_proposals.png\")\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(img_display)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(output_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "    # Log image as artifact\n",
    "    img_artifact = wandb.Artifact(f\"rpn_proposals_{base_name}\", type=\"image\")\n",
    "    img_artifact.add_file(output_path)\n",
    "    wandb.log_artifact(img_artifact)\n",
    "\n",
    "    print(f\"Saved and logged proposal visualization to: {output_path}\")\n",
    "\n",
    "# ----------------------------\n",
    "#  RUN VISUALIZATION\n",
    "# ----------------------------\n",
    "TEST_IMAGE_PATH = '../dataset/images/airplane/image_0002.jpg'\n",
    "visualize_rpn_proposals(TEST_IMAGE_PATH, rpn_model, backbone)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
