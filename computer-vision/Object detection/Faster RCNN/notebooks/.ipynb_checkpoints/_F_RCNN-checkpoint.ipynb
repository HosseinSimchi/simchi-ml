{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a1006c3-9e79-4f93-8e50-c46dfd203eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator, RPNHead\n",
    "from torchvision.ops import RoIAlign, RoIPool, MultiScaleRoIAlign\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3828afe3-c442-4d37-ae39-e7ecee589db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53fec2f3-76a8-42ad-9c1f-43634e45aef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = '../dataset/images'\n",
    "ANNOTATIONS_DIR = '../dataset/annotations'\n",
    "TARGET_SIZE = (224, 224)\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f254c7c-7e2c-480a-b393-d4f84f9a9939",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [d for d in os.listdir(IMAGE_DIR) if os.path.isdir(os.path.join(IMAGE_DIR, d))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d997d23b-3d5b-428e-82a5-44ff618f1b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [f for f in os.listdir(ANNOTATIONS_DIR) if f.endswith('.csv')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b588e8c-888d-4996-aac7-80632109478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {name: i + 1 for i, name in enumerate(class_names)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b1f74d6-465c-49ec-b829-c88bfcf468d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "️Invalid box found and removed in '../dataset/images/airplane/image_0118.jpg': [np.int64(104), np.int64(38), np.int64(126), np.int64(38)]\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "\n",
    "for i in range(len(class_names)):\n",
    "    class_name = class_names[i]\n",
    "    class_dir = os.path.join(IMAGE_DIR, class_name)\n",
    "    csv_file_name = csv_files[i]\n",
    "    \n",
    "    csv_path = os.path.join(ANNOTATIONS_DIR, csv_file_name)\n",
    "    df_annotations = pd.read_csv(csv_path)\n",
    "\n",
    "    for image_name in os.listdir(class_dir):\n",
    "        image_path = os.path.join(class_dir, image_name)\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            continue\n",
    "                \n",
    "        h, w, _ = image.shape\n",
    "\n",
    "        row = df_annotations[df_annotations['image_name'] == image_name]\n",
    "\n",
    "        # ---- FIX HERE ----\n",
    "        if row.empty:\n",
    "            continue\n",
    "        # -------------------\n",
    "\n",
    "        ann = row.iloc[0, 1:].tolist()\n",
    "\n",
    "        if ann[2] > ann[0] and ann[3] > ann[1]:\n",
    "            ann[0] = int((ann[0] / w) * TARGET_SIZE[0])\n",
    "            ann[1] = int((ann[1] / h) * TARGET_SIZE[0])\n",
    "            ann[2] = int((ann[2] / w) * TARGET_SIZE[0])\n",
    "            ann[3] = int((ann[3] / h) * TARGET_SIZE[0])\n",
    "        \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = cv2.resize(image, TARGET_SIZE)\n",
    "            \n",
    "            image_tensor = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "            label_tensor = torch.tensor([label_map[class_name]], dtype=torch.int64)\n",
    "            ann_tensor = torch.tensor([ann], dtype=torch.float32)\n",
    "    \n",
    "            target = {\n",
    "                'boxes': ann_tensor,\n",
    "                'labels': label_tensor\n",
    "            }\n",
    "            dataset.append((image_tensor, target))\n",
    "        else:\n",
    "            print(f\"️Invalid box found and removed in '{image_path}': {ann}\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c44534d8-0921-404b-acb6-c35b28821e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec88b08b-a77e-4c2a-9e91-1a65b6529008",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator().manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ad6729b-7017-4f80-9248-675935fc85d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size], generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04f02027-83a3-4893-914a-0f3e909d0502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset (1669)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset ({len(dataset)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b7cd344f-4bc7-4fe9-b951-274bc1911da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (166), Test (1503)\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Train ({len(train_dataset)}), Test ({len(test_dataset)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "816667ce-992e-4b99-a938-fd733c41a808",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = torchvision.models.resnet18()\n",
    "backbone = torch.nn.Sequential(*list(resnet_model.children())[:-2])\n",
    "backbone.out_channels = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53d534c7-906a-46a7-9bce-07bc6c44deae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in backbone.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4107609-2112-47fb-a9d9-c9e9691873cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_generator = AnchorGenerator(\n",
    "    sizes=((32, 64, 128),),\n",
    "    aspect_ratios=((0.5, 1.0, 2.0),)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3c5ba181-1199-41b0-a6c3-06861507ba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_pool = MultiScaleRoIAlign(\n",
    "    featmap_names=['0'],\n",
    "    output_size=7,\n",
    "    sampling_ratio=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e56f5f95-a57a-460e-853a-6ca7e08a73b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = backbone.out_channels\n",
    "num_anchors = anchor_generator.num_anchors_per_location()[0]\n",
    "\n",
    "rpn_head = RPNHead(in_channels=in_channels, num_anchors=num_anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f01f6c9e-c811-43f9-a9c7-43c164d5ed99",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(class_names) + 1\n",
    "\n",
    "model = FasterRCNN(\n",
    "    backbone,\n",
    "    num_classes = NUM_CLASSES,\n",
    "    rpn_anchor_generator = anchor_generator,\n",
    "    rpn_head=rpn_head,\n",
    "    box_roi_pool = roi_pool\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "965e8199-5e55-42ea-b781-370d4f4790b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e28ec1-907e-4b8e-8de2-e9407d8a80fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "LOG_DIR = \"../model_outputs\"\n",
    "LOG_FILE = os.path.join(LOG_DIR, \"train_log.txt\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# Open log file in append mode\n",
    "log_f = open(LOG_FILE, \"a\")\n",
    "\n",
    "backbone.to(DEVICE)\n",
    "model.to(DEVICE)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_losses = []\n",
    "    print(f\"Start Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    log_f.write(f\"Start Epoch {epoch+1}/{NUM_EPOCHS}\\n\")\n",
    "\n",
    "    for i, (images, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        images_gpu = [img.to(DEVICE) for img in images]\n",
    "        targets_gpu = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "        loss_dict = model(images_gpu, targets_gpu)\n",
    "        losses = sum(loss_dict.values())\n",
    "        \n",
    "        if torch.isfinite(losses):\n",
    "            epoch_losses.append(losses.item())\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    if epoch_losses:\n",
    "        mean_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "        msg = f\"Epoch {epoch+1} | Loss: {mean_loss:.4f}\"\n",
    "        print(msg)\n",
    "        log_f.write(msg + \"\\n\")\n",
    "    else:\n",
    "        msg = f\"Epoch {epoch+1} | No valid losses\"\n",
    "        print(msg)\n",
    "        log_f.write(msg + \"\\n\")\n",
    "\n",
    "# Close the log file when training is finished\n",
    "log_f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7defcf29-7d11-4d85-a28d-02f18c684048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
